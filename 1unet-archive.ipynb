{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a9468-334f-4f06-bc71-a111476a8d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics opencv-python numpy pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069305d-2c90-4a61-a7a8-9365d4021b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch torch torchvision albumentations\n",
    "!pip install geopandas rasterio scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164863ad-0847-4e4b-aac1-e2aac266dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation file...\n",
      "Creating and saving segmentation masks...\n",
      "\n",
      "Data preparation complete.\n",
      "Images and masks saved to: data\\processed_unet\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "from shapely.geometry import Polygon\n",
    "from PIL import Image\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "RAW_DATA_DIR = Path(\"data/raw\")\n",
    "PROCESSED_DATA_DIR = Path(\"data/processed_unet\")\n",
    "SOLAFUNE_JSON_PATH = RAW_DATA_DIR / \"train_annotations.json\" # Assumed name\n",
    "\n",
    "# --- MAIN SCRIPT ---\n",
    "\n",
    "def parse_solafune_json_for_polygons(json_path):\n",
    "    \"\"\"\n",
    "    Parses the proprietary Solafune JSON to extract polygons.\n",
    "    This version is adapted for the user-provided JSON structure where\n",
    "    annotations are nested within each image object.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Maps image filenames to their size and a list of polygons.\n",
    "              Example: {'image1.png': {'size': (W, H), 'polygons': [Polygon(...),...]}}\n",
    "    \"\"\"\n",
    "    annotations = {}\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Iterate through the list of image objects\n",
    "    for image_data in data.get('images',):\n",
    "        filename = image_data['file_name']\n",
    "        width = image_data['width']\n",
    "        height = image_data['height']\n",
    "        \n",
    "        # Initialize the entry for this image\n",
    "        annotations[filename] = {\n",
    "            'size': (width, height),\n",
    "            'polygons': [] \n",
    "        }\n",
    "        \n",
    "        # Iterate through the annotations nested within this image\n",
    "        for ann in image_data.get('annotations',):\n",
    "            # The segmentation is a flat list [x1, y1, x2, y2,...]\n",
    "            flat_points = ann['segmentation']\n",
    "            \n",
    "            # Convert flat list to a list of (x, y) coordinate pairs\n",
    "            points = list(zip(flat_points[::2], flat_points[1::2]))\n",
    "            \n",
    "            # A valid polygon needs at least 3 points\n",
    "            if len(points) >= 3:\n",
    "                annotations[filename]['polygons'].append(Polygon(points))\n",
    "                \n",
    "    return annotations\n",
    "\n",
    "def create_and_save_masks(all_annotations, raw_image_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Creates binary masks from polygons and saves them as PNG files.\n",
    "    \"\"\"\n",
    "    print(\"Creating and saving segmentation masks...\")\n",
    "    images_out_dir = output_dir / \"images\"\n",
    "    masks_out_dir = output_dir / \"masks\"\n",
    "    images_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    masks_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for filename, data in all_annotations.items():\n",
    "        # Copy original image to new directory\n",
    "        shutil.copy(raw_image_dir / filename, images_out_dir / filename)\n",
    "\n",
    "        # Create mask\n",
    "        width, height = data['size']\n",
    "        polygons = data['polygons']\n",
    "        \n",
    "        if not polygons:\n",
    "            mask_array = np.zeros((height, width), dtype=np.uint8)\n",
    "        else:\n",
    "            # Rasterize the polygons into a mask array\n",
    "            mask_array = features.rasterize(\n",
    "                shapes=polygons,\n",
    "                out_shape=(height, width),\n",
    "                transform=rasterio.Affine.identity(),\n",
    "                fill=0,\n",
    "                all_touched=True,\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "        \n",
    "        # Save mask as a grayscale PNG image\n",
    "        mask_image = Image.fromarray(mask_array * 255) # Scale to 0-255 for image format\n",
    "        mask_image.save(masks_out_dir / filename)\n",
    "\n",
    "def main():\n",
    "    # --- 1. Parse Annotations ---\n",
    "    print(\"Parsing annotation file...\")\n",
    "    # Assumes raw images are in 'data/raw/train_images'\n",
    "    raw_image_dir = RAW_DATA_DIR / \"train_images\"\n",
    "    all_annotations = parse_solafune_json_for_polygons(SOLAFUNE_JSON_PATH)\n",
    "    if not all_annotations:\n",
    "        print(\"Parsing failed. Check the `parse_solafune_json_for_polygons` function.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Create and Save Masks ---\n",
    "    create_and_save_masks(all_annotations, raw_image_dir, PROCESSED_DATA_DIR)\n",
    "\n",
    "    # --- 3. Split into Train/Validation sets ---\n",
    "    # (This part can be handled by the PyTorch Dataset/DataLoader instead of moving files)\n",
    "    print(\"\\nData preparation complete.\")\n",
    "    print(f\"Images and masks saved to: {PROCESSED_DATA_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb94638-beb7-414f-8aaa-2cb7d9a83e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hmanasi1\\Documents\\ADML\\Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Compose.__init__() missing 1 required positional argument: 'transforms'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m BATCH_SIZE = \u001b[32m8\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# --- 1. DEFINE AUGMENTATION PIPELINES ---\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Define a strong set of augmentations for the training set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m train_transform = \u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# For validation, we only need to normalize and convert to a tensor\u001b[39;00m\n\u001b[32m     25\u001b[39m val_transform = A.Compose()\n",
      "\u001b[31mTypeError\u001b[39m: Compose.__init__() missing 1 required positional argument: 'transforms'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import albumentations as A # <-- Import Albumentations\n",
    "from albumentations.pytorch import ToTensorV2 # <-- Import the PyTorch tensor converter\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = Path(\"data/processed_unet\")\n",
    "ENCODER = 'resnet34'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# --- 1. DEFINE AUGMENTATION PIPELINES ---\n",
    "# Define a strong set of augmentations for the training set\n",
    "train_transform = A.Compose()\n",
    "\n",
    "# For validation, we only need to normalize and convert to a tensor\n",
    "val_transform = A.Compose()\n",
    "\n",
    "\n",
    "# --- 2. UPDATE PYTORCH DATASET CLASS ---\n",
    "class TreeCanopyDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx]).convert(\"L\"), dtype=np.float32)\n",
    "        \n",
    "        # The mask should be binary (0.0 or 1.0)\n",
    "        mask = mask / 255.0\n",
    "        \n",
    "        if self.transform:\n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask'].unsqueeze(0) # Add channel dimension for the mask\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# --- MAIN SCRIPT (MODIFIED PART) ---\n",
    "def main():\n",
    "    # --- 1. Prepare Data Paths and Splits ---\n",
    "    all_image_paths = sorted(list((DATA_DIR / \"images\").glob(\"*.png\")))\n",
    "    all_mask_paths = sorted(list((DATA_DIR / \"masks\").glob(\"*.png\")))\n",
    "\n",
    "    train_imgs, val_imgs, train_msks, val_msks = train_test_split(\n",
    "        all_image_paths, all_mask_paths, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Apply the respective transforms to the datasets\n",
    "    train_dataset = TreeCanopyDataset(train_imgs, train_msks, transform=train_transform)\n",
    "    val_dataset = TreeCanopyDataset(val_imgs, val_msks, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    #... (The rest of the training loop remains the same)...\n",
    "    \n",
    "    # --- 2. Create Model ---\n",
    "    model = smp.Unet(\n",
    "        encoder_name=ENCODER, \n",
    "        encoder_weights=ENCODER_WEIGHTS,\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "        activation='sigmoid',\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # --- 3. Define Loss and Optimizer ---\n",
    "    loss = smp.losses.DiceLoss(mode='binary')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # --- 4. Training Loop ---\n",
    "    print(f\"Starting training on {DEVICE}...\")\n",
    "    best_iou = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            l = loss(outputs, masks)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += l.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_iou = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(outputs, masks.long(), mode='binary')\n",
    "                iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction='micro')\n",
    "                val_iou += iou.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        avg_val_iou = val_iou / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_train_loss:.4f}, Val IoU: {avg_val_iou:.4f}\")\n",
    "\n",
    "        if avg_val_iou > best_iou:\n",
    "            best_iou = avg_val_iou\n",
    "            torch.save(model.state_dict(), 'best_unet_model.pth')\n",
    "            print(\"   -> Best model saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16673398-e1fb-4038-909e-ef314a8dbcd3",
   "metadata": {},
   "source": [
    "# Catgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d19a66f-4269-412e-a56d-4b15f083b576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and saving segmentation masks...\n",
      "\n",
      "Data preparation complete.\n",
      "Images and masks saved to: data\\processed_unet\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from rasterio import features\n",
    "import rasterio\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "RAW_DATA_DIR = Path(\"data/raw\")\n",
    "PROCESSED_DATA_DIR = Path(\"data/processed_unet\")\n",
    "SOLAFUNE_JSON_PATH = RAW_DATA_DIR / \"train_annotations.json\"\n",
    "\n",
    "# --- MAIN SCRIPT ---\n",
    "\n",
    "def parse_solafune_json_for_polygons(json_path):\n",
    "    \"\"\"\n",
    "    Parses the proprietary Solafune JSON to extract polygons.\n",
    "    Returns a dict: {'image1.png': {'size': (W,H), 'polygons': [Polygon(...), ...]}}\n",
    "    \"\"\"\n",
    "    annotations = {}\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for image_data in data.get('images', []):\n",
    "        filename = image_data['file_name']\n",
    "        width = image_data['width']\n",
    "        height = image_data['height']\n",
    "        \n",
    "        annotations[filename] = {'size': (width, height), 'polygons': []}\n",
    "        \n",
    "        for ann in image_data.get('annotations', []):\n",
    "            flat_points = ann['segmentation']\n",
    "            points = list(zip(flat_points[::2], flat_points[1::2]))\n",
    "            if len(points) >= 3:\n",
    "                annotations[filename]['polygons'].append(Polygon(points))\n",
    "                \n",
    "    return annotations\n",
    "\n",
    "def create_and_save_masks(all_annotations, raw_image_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Creates binary masks from polygons and saves them as PNGs.\n",
    "    \"\"\"\n",
    "    print(\"Creating and saving segmentation masks...\")\n",
    "    images_out_dir = output_dir / \"images\"\n",
    "    masks_out_dir = output_dir / \"masks\"\n",
    "    images_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    masks_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for filename, data in all_annotations.items():\n",
    "        shutil.copy(raw_image_dir / filename, images_out_dir / filename)\n",
    "        width, height = data['size']\n",
    "        polygons = data['polygons']\n",
    "        \n",
    "        if not polygons:\n",
    "            mask_array = np.zeros((height, width), dtype=np.uint8)\n",
    "        else:\n",
    "            mask_array = features.rasterize(\n",
    "                shapes=polygons,\n",
    "                out_shape=(height, width),\n",
    "                transform=rasterio.Affine.identity(),\n",
    "                fill=0,\n",
    "                all_touched=True,\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "        mask_image = Image.fromarray(mask_array * 255)\n",
    "        mask_image.save(masks_out_dir / filename)\n",
    "\n",
    "def main():\n",
    "    raw_image_dir = RAW_DATA_DIR / \"train_images\"\n",
    "    all_annotations = parse_solafune_json_for_polygons(SOLAFUNE_JSON_PATH)\n",
    "    if not all_annotations:\n",
    "        print(\"Parsing failed. Check JSON file.\")\n",
    "        return\n",
    "\n",
    "    create_and_save_masks(all_annotations, raw_image_dir, PROCESSED_DATA_DIR)\n",
    "\n",
    "    print(\"\\nData preparation complete.\")\n",
    "    print(f\"Images and masks saved to: {PROCESSED_DATA_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5888a-02b2-4cf3-a086-31209d3c1915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found: 150\n",
      "Number of masks found: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hmanasi1\\Documents\\ADML\\Project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hmanasi1\\.cache\\huggingface\\hub\\models--smp-hub--resnet34.imagenet. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = Path(\"data/processed_unet\")\n",
    "ENCODER = 'resnet34'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# --- 1. AUGMENTATION PIPELINES ---\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# --- 2. PYTORCH DATASET ---\n",
    "class TreeCanopyDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx]).convert(\"L\"), dtype=np.float32) / 255.0\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask'].unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# --- 3. TRAINING LOOP ---\n",
    "def main():\n",
    "    all_image_paths = sorted(list((DATA_DIR / \"images\").glob(\"*.tif\")))\n",
    "    all_mask_paths = sorted(list((DATA_DIR / \"masks\").glob(\"*.tif\")))\n",
    "\n",
    "    print(\"Number of images found:\", len(all_image_paths))\n",
    "    print(\"Number of masks found:\", len(all_mask_paths))\n",
    "\n",
    "    train_imgs, val_imgs, train_msks, val_msks = train_test_split(\n",
    "        all_image_paths, all_mask_paths, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = TreeCanopyDataset(train_imgs, train_msks, transform=train_transform)\n",
    "    val_dataset = TreeCanopyDataset(val_imgs, val_msks, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Model, Loss, Optimizer\n",
    "    model = smp.Unet(encoder_name=ENCODER, encoder_weights=ENCODER_WEIGHTS,\n",
    "                     in_channels=3, classes=1, activation='sigmoid')\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    loss_fn = smp.losses.DiceLoss(mode='binary')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    print(f\"Starting training on {DEVICE}...\")\n",
    "    best_iou = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_iou = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(outputs, masks.long(), mode='binary')\n",
    "                val_iou += smp.metrics.iou_score(tp, fp, fn, tn, reduction='micro').item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        avg_val_iou = val_iou / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_train_loss:.4f}, Val IoU: {avg_val_iou:.4f}\")\n",
    "\n",
    "        if avg_val_iou > best_iou:\n",
    "            best_iou = avg_val_iou\n",
    "            torch.save(model.state_dict(), 'best_unet_model.pth')\n",
    "            print(\"   -> Best model saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924dd3be-98ac-41c4-b833-e50579b6a211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (adml)",
   "language": "python",
   "name": "adml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
